# üåç Jobs ‚Äì ETL Pipeline & Analysis

A complete ETL pipeline to extract, clean, and analyze remote job listings for IT & non IT roles across regions.

1. Introduction
This project implements an ETL pipeline to collect, clean, and analyze remote job listings from two public APIs (Jobicy and Remotive). It targets data science and engineering roles and focuses on geographical diversity.
2. Objectives
- Create a modular ETL system.
- Enrich data by combining multiple sources.
- Apply data cleaning and imputation techniques.
- Store structured data in PostgreSQL.
- Enable analysis and visualization in R and Tableau.
1. ETL Pipeline Description
- **Extraction**: Job data is retrieved using `requests` from the two APIs and saved as CSV files.
- **Transformation**:
    - Columns were cleaned and standardized.
    - Missing salary values were imputed using PMM (Predictive Mean Matching), implemented via `IterativeImputer` and `RandomForestRegressor` from `sklearn`. This method was chosen for its ability to preserve realistic value distributions.
    - I made sure that the imputation did not distort the original data distribution by using **boxplots and histograms** to compare salary distributions **before and after imputation**.
    - Data was grouped and labeled by region, seniority, and title.
    - Duplicate handling ensured unique jobs per company and title.
- **Loading**: The cleaned dataset is loaded into a PostgreSQL database under the schema `jobs.all_jobs`.
1. Analysis and Visualization Tools
- **R** was used for exploratory data analysis and potential modeling.
1. Key Decisions and Rationale
- Combining Jobicy and Remotive ensures broader coverage of global job markets.
- Imputing salaries instead of deleting rows prevents bias and loss of information.
- Splitting data by region, title, and seniority allows finer granularity in analysis.

---

[ESPA√ëOL]

1. Introducci√≥n
Este proyecto implementa un pipeline ETL para recopilar, limpiar y analizar ofertas de empleo remoto desde dos APIs p√∫blicas (Jobicy y Remotive). Est√° orientado a roles en ciencia y an√°lisis de datos, con √©nfasis en diversidad geogr√°fica.
2. Objetivos
- Crear un sistema ETL modular.
- Enriquecer datos combinando m√∫ltiples fuentes.
- Aplicar t√©cnicas de limpieza e imputaci√≥n.
- Almacenar datos estructurados en PostgreSQL.
- Habilitar el an√°lisis y visualizaci√≥n en R y Tableau.
1. Descripci√≥n del Pipeline
- **Extracci√≥n**: Los datos se obtienen mediante `requests` desde ambas APIs y se guardan en CSV.
- **Transformaci√≥n**:
    - Se limpian y estandarizan las columnas.
    - Se imputan salarios faltantes usando PMM (Predictive Mean Matching), con `IterativeImputer` y `RandomForestRegressor`. Este m√©todo preserva la distribuci√≥n realista de los valores.
    - Me asegur√© de que la imputaci√≥n no distorsionara la distribuci√≥n original de los datos mediante **gr√°ficos de boxplot e histogramas** que comparan los salarios **antes y despu√©s de la imputaci√≥n**.
    - Se agrupan datos por regi√≥n, seniority y t√≠tulo.
    - Se controlan duplicados por empresa y t√≠tulo.
- **Carga**: El dataset limpio se carga a una base de datos PostgreSQL bajo el esquema `jobs.all_jobs`.
1. Herramientas de An√°lisis y Visualizaci√≥n
- **R** para an√°lisis estad√≠stico exploratorio y modelado.
1. Decisiones Clave
- Usar dos fuentes garantiza cobertura m√°s amplia.
- Imputar en lugar de eliminar evita sesgos.
- Separar por regi√≥n, t√≠tulo y seniority mejora la granularidad del an√°lisis.